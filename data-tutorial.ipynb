{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direction-Following Dataset Generator Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use the two/four-directional following dataset generator to create custom datasets for testing directional reasoning capabilities. The generator creates stories about actors moving in different directions, and tasks to determine whether two actors end up facing the same direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "from data.dataset_generator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating Datasets\n",
    "\n",
    "The direction-following dataset consists of stories where actors move in various directions and interact with each other. Each story ends with a query asking if two selected actors are facing the same direction.\n",
    "\n",
    "You can generate datasets in three ways:\n",
    "1. **Single dataset**: Generate a dataset with specific parameters (density, actor counts, etc.)\n",
    "2. **Experiment datasets**: Generate all datasets defined in the experiment\n",
    "3. **Merge datasets**: Combine multiple datasets with the same direction type\n",
    "\n",
    "Below, we'll demonstrate how to generate all experiment datasets and then how to merge them by direction type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.seed(42)\n",
    "\n",
    "# define desired output directory\n",
    "output_dir = \"./datasets_new\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\"\"\"\n",
    "# Example: Generate a single dataset for a specific density \n",
    "print(\"Generating simple_4dir dataset...\")\n",
    "dataset_path = generate_complete_dataset(\n",
    "    train_size=200,\n",
    "    valid_size=50,\n",
    "    validB_size=100,\n",
    "    test_size=100,\n",
    "    n_directions=4,\n",
    "    target_densities=[0.26],  # simple density\n",
    "    balance_mode=\"actor_only\",\n",
    "    output_dir=output_dir,\n",
    "    output_name=\"simple_4dir\"\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Example : Generate all datasets with stats matching the experiment\n",
    "print(\"\\nGenerating all datasets...\")\n",
    "generate_experiment_datasets(output_dir=output_dir)\n",
    "\n",
    "print(\"\\nAll datasets generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspecting Generated Datasets\n",
    "\n",
    "After generating the datasets, we should verify they have the expected properties:\n",
    "- Correct number of stories in each split\n",
    "- Proper distribution of actor counts\n",
    "- Balanced density categories\n",
    "- Appropriate number of positive/negative examples\n",
    "\n",
    "The function below checks basic statistics for each dataset file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_dataset_file(file_path):\n",
    "    \"\"\"Print summary statistics for all dataset files\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Track stories by split\n",
    "    train_count = 0\n",
    "    valid_count = 0\n",
    "    validb_count = 0\n",
    "    test_count = 0\n",
    "    \n",
    "    for key, group in data.items():\n",
    "        if isinstance(key, tuple) and len(key) > 0:\n",
    "            split = key[0].lower() if isinstance(key[0], str) else None\n",
    "            \n",
    "            # Count stories in this group\n",
    "            total = len(group[\"pos\"]) + len(group[\"neg\"])\n",
    "            \n",
    "            if split == 'train':\n",
    "                train_count += total\n",
    "            elif split == 'valid':\n",
    "                valid_count += total\n",
    "            elif split == 'validb':\n",
    "                validb_count += total\n",
    "            elif split == 'test':\n",
    "                test_count += total\n",
    "    \n",
    "    print(f\"Dataset file: {os.path.basename(file_path)}\")\n",
    "    print(f\"  Total stories: {train_count + valid_count + validb_count + test_count}\")\n",
    "    print(f\"  Train: {train_count} stories\")\n",
    "    print(f\"  Valid: {valid_count} stories\")\n",
    "    print(f\"  ValidB: {validb_count} stories\")\n",
    "    print(f\"  Test: {test_count} stories\")\n",
    "    print()\n",
    "\n",
    "dataset_dir = \"./datasets_new\"\n",
    "for dataset_file in os.listdir(dataset_dir):\n",
    "    if dataset_file.endswith('.pkl') and not dataset_file.startswith('all_'):\n",
    "        check_dataset_file(os.path.join(dataset_dir, dataset_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merging Datasets by Direction Type\n",
    "\n",
    "To train the models on all densities combined, using the predetermined train/valid indices, we want to combine all datasets of the same direction type (2dir or 4dir).\n",
    "The `merge_datasets_by_direction` function combines multiple datasets while maintaining split information.\n",
    "\n",
    "This creates combined files like:\n",
    "- `all_2dir.pkl` - All 2-direction datasets combined\n",
    "- `all_4dir.pkl` - All 4-direction datasets combined\n",
    "\n",
    "It also generates appropriate index files for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging separate density datasets into complete all_2dir and all_4dir datasets\n",
    "\n",
    "output_dir = \"./datasets_new\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Available datasets:\")\n",
    "all_dataset_files = glob.glob(os.path.join(output_dir, \"*.pkl\"))\n",
    "all_dataset_files = [f for f in all_dataset_files if not os.path.basename(f).startswith(\"all_\") and not os.path.basename(f).endswith(\"metadata.pkl\")]\n",
    "for dataset_file in all_dataset_files:\n",
    "    print(f\" - {os.path.basename(dataset_file)}\")\n",
    "\n",
    "# merge 2dir datasets\n",
    "print(\"\\nMerging 2-directional datasets:\")\n",
    "config_2dir = merge_datasets_by_direction(output_dir, \"2dir\")\n",
    "\n",
    "# merge 4dir datasets\n",
    "print(\"\\nMerging 4-directional datasets:\")\n",
    "config_4dir = merge_datasets_by_direction(output_dir, \"4dir\")\n",
    "\n",
    "print(\"\\nSummary of merged datasets:\")\n",
    "if config_2dir:\n",
    "    print(f\"Created all_2dir dataset:\")\n",
    "    print(f\" - Stories file: {os.path.basename(config_2dir['stories_file'])}\")\n",
    "    print(f\" - Train indices: {os.path.basename(config_2dir['train_indices_file'])}\")\n",
    "    print(f\" - ValidA indices: {os.path.basename(config_2dir['valid_indices_file'])}\")\n",
    "    print(f\" - ValidB indices: {os.path.basename(config_2dir['validb_indices_file'])}\")\n",
    "    print(f\" - Test indices: {os.path.basename(config_2dir['test_indices_file'])}\")\n",
    "\n",
    "if config_4dir:\n",
    "    print(f\"\\nCreated all_4dir dataset:\")\n",
    "    print(f\" - Stories file: {os.path.basename(config_4dir['stories_file'])}\")\n",
    "    print(f\" - Train indices: {os.path.basename(config_4dir['train_indices_file'])}\")\n",
    "    print(f\" - ValidA indices: {os.path.basename(config_4dir['valid_indices_file'])}\")\n",
    "    print(f\" - ValidB indices: {os.path.basename(config_4dir['validb_indices_file'])}\")\n",
    "    print(f\" - Test indices: {os.path.basename(config_4dir['test_indices_file'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detailed Dataset Analysis\n",
    "\n",
    "Now let's examine the actor count distribution in more detail.\n",
    "This is important to verify that our dataset generation maintained the expected actor ranges:\n",
    "- Train/Valid: 2-8 actors\n",
    "- ValidB: 9-20 actors  \n",
    "- Test: 21-30 actors\n",
    "\n",
    "We'll use the `inspect_dataset_actor_counts` function to check each dataset, then \n",
    "look at a specific dataset (simple_4dir) in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from debugging_functions import inspect_dataset_actor_counts\n",
    "\n",
    "output_dir = \"./datasets_new\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Available datasets:\")\n",
    "all_dataset_files = glob.glob(os.path.join(output_dir, \"*.pkl\"))\n",
    "# filter out merged datasets and metadata files\n",
    "dataset_files = [f for f in all_dataset_files if not os.path.basename(f).startswith(\"all_\") and os.path.basename(f).endswith(\".pkl\")]\n",
    "for dataset_file in sorted(dataset_files):\n",
    "    print(f\" - {os.path.basename(dataset_file)}\")\n",
    "    inspect_dataset_actor_counts(dataset_file)\n",
    "\n",
    "simple_4dir_path = os.path.join(output_dir, \"simple_4dir.pkl\")\n",
    "if os.path.exists(simple_4dir_path):\n",
    "    print(\"\\nInspecting simple_4dir dataset in detail:\")\n",
    "    \n",
    "    with open(simple_4dir_path, 'rb') as f:\n",
    "        simple_data = pickle.load(f)\n",
    "    \n",
    "    valid_actors = []\n",
    "    for key, group in simple_data.items():\n",
    "        if isinstance(key, tuple) and len(key) > 0 and key[0] == 'valid':\n",
    "            for story_type in ['pos', 'neg']:\n",
    "                for story in group[story_type]:\n",
    "                    if len(story) > 2 and isinstance(story[2], dict):\n",
    "                        if 'num_actors' in story[2]:\n",
    "                            valid_actors.append(story[2]['num_actors'])\n",
    "    \n",
    "    print(\"Actor count distribution in simple_4dir validation set:\")\n",
    "    for actor_count in range(2, 9):\n",
    "        count = valid_actors.count(actor_count)\n",
    "        print(f\"  - {actor_count} actors: {count} stories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verifying Merged Datasets\n",
    "\n",
    "Finally, let's verify that our merged datasets maintain the correct split information and actor ranges.\n",
    "The `check_merged_dataset_stats` function validates:\n",
    "- Index file integrity\n",
    "- Actor counts for each split\n",
    "- Category distribution \n",
    "- Whether stories are within their expected actor ranges\n",
    "\n",
    "This helps ensure the merged datasets are properly structured for experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_merged_dataset_stats(stories_file, indices_files, all_stories_file=None):\n",
    "    \"\"\"\n",
    "    Check statistics of a merged dataset and its indices files.\n",
    "    \n",
    "    Args:\n",
    "        stories_file (str): Path to the merged dataset pickle file\n",
    "        indices_files (dict): Dictionary of split names to index file paths\n",
    "        all_stories_file (str, optional): Path to the all_stories list file for direct indexing\n",
    "    \"\"\"\n",
    "    print(f\"\\nChecking statistics for {os.path.basename(stories_file)}\")\n",
    "    \n",
    "    with open(stories_file, 'rb') as f:\n",
    "        combined_stories = pickle.load(f)\n",
    "    \n",
    "    all_stories = []\n",
    "    if all_stories_file and os.path.exists(all_stories_file):\n",
    "        print(f\"Using all_stories from {os.path.basename(all_stories_file)}\")\n",
    "        with open(all_stories_file, 'rb') as f:\n",
    "            all_stories = pickle.load(f)\n",
    "    else:\n",
    "        # reconstruct from combined_stories\n",
    "        print(\"Reconstructing all_stories from combined dataset\")\n",
    "        for key, group in combined_stories.items():\n",
    "            for story_type in [\"pos\", \"neg\"]:\n",
    "                for story in group[story_type]:\n",
    "                    all_stories.append(story)\n",
    "    \n",
    "    story_metadata = {}\n",
    "    for idx, story in enumerate(all_stories):\n",
    "        if len(story) > 2 and isinstance(story[2], dict):\n",
    "            story_metadata[idx] = story[2]\n",
    "    \n",
    "    print(f\"Total stories in dataset: {len(all_stories)}\")\n",
    "    \n",
    "    # check each split's indices\n",
    "    for split_name, indices_file in indices_files.items():\n",
    "        if not os.path.exists(indices_file):\n",
    "            print(f\"  Warning: Indices file {indices_file} not found\")\n",
    "            continue\n",
    "        \n",
    "        # load indices\n",
    "        with open(indices_file, 'r') as f:\n",
    "            indices = json.load(f)\n",
    "        \n",
    "        # define expected actor ranges\n",
    "        actor_ranges = {\n",
    "            'train': (2, 8),\n",
    "            'valid': (2, 8),\n",
    "            'validB': (9, 20),\n",
    "            'test': (21, 30)\n",
    "        }\n",
    "        min_actors, max_actors = actor_ranges.get(split_name.lower(), (0, 100))\n",
    "        \n",
    "        # count stories by actor count and category for this split\n",
    "        split_counts = {\n",
    "            'actor_counts': defaultdict(int),\n",
    "            'category_counts': defaultdict(int),\n",
    "            'valid_indices': 0,\n",
    "            'invalid_indices': 0,\n",
    "            'in_range': 0,\n",
    "            'out_range': 0\n",
    "        }\n",
    "        \n",
    "        # process each index\n",
    "        for idx in indices:\n",
    "            if idx < 0 or idx >= len(all_stories):\n",
    "                split_counts['invalid_indices'] += 1\n",
    "                continue\n",
    "            \n",
    "            split_counts['valid_indices'] += 1\n",
    "            \n",
    "            story = all_stories[idx]\n",
    "            if len(story) > 2 and isinstance(story[2], dict):\n",
    "                metadata = story[2]\n",
    "                \n",
    "                if 'num_actors' in metadata:\n",
    "                    actor_count = metadata['num_actors']\n",
    "                    split_counts['actor_counts'][actor_count] += 1\n",
    "                    \n",
    "                    # check if in expected range\n",
    "                    if min_actors <= actor_count <= max_actors:\n",
    "                        split_counts['in_range'] += 1\n",
    "                    else:\n",
    "                        split_counts['out_range'] += 1\n",
    "                \n",
    "                if 'category' in metadata:\n",
    "                    category = metadata['category']\n",
    "                    split_counts['category_counts'][category] += 1\n",
    "        \n",
    "        print(f\"\\n{split_name.upper()} SPLIT:\")\n",
    "        print(f\"  Valid indices: {split_counts['valid_indices']} ({split_counts['valid_indices']/max(1, len(indices))*100:.1f}%)\")\n",
    "        \n",
    "        if split_counts['invalid_indices'] > 0:\n",
    "            print(f\"  Invalid indices: {split_counts['invalid_indices']} ({split_counts['invalid_indices']/max(1, len(indices))*100:.1f}%)\")\n",
    "            \n",
    "        print(f\"  Actor range validity:\")\n",
    "        print(f\"    - Within expected range ({min_actors}-{max_actors}): {split_counts['in_range']} ({split_counts['in_range']/max(1, split_counts['valid_indices'])*100:.1f}%)\")\n",
    "        print(f\"    - Outside expected range: {split_counts['out_range']} ({split_counts['out_range']/max(1, split_counts['valid_indices'])*100:.1f}%)\")\n",
    "        \n",
    "        print(\"  Actor counts:\")\n",
    "        for actor_count in sorted(split_counts['actor_counts'].keys()):\n",
    "            count = split_counts['actor_counts'][actor_count]\n",
    "            in_range = min_actors <= actor_count <= max_actors\n",
    "            range_status = \"✓\" if in_range else \"✗\"\n",
    "            print(f\"    - {actor_count} actors: {count} stories ({count/max(1, split_counts['valid_indices'])*100:.1f}%) {range_status}\")\n",
    "        \n",
    "        print(\"  Category distribution:\")\n",
    "        for category, count in sorted(split_counts['category_counts'].items()):\n",
    "            print(f\"    - {category}: {count} stories ({count/max(1, split_counts['valid_indices'])*100:.1f}%)\")\n",
    "\n",
    "\n",
    "check_merged_dataset_stats(\n",
    "    \"./datasets_new/all_4dir.pkl\", \n",
    "    {\n",
    "        'train': \"./datasets_new/train_indices_all_4dir.json\",\n",
    "        'valid': \"./datasets_new/valid_indices_all_4dir.json\",\n",
    "        'validB': \"./datasets_new/validb_indices_all_4dir.json\",\n",
    "        'test': \"./datasets_new/test_indices_all_4dir.json\"\n",
    "    },\n",
    "    all_stories_file=\"./datasets_new/all_stories_4dir.pkl\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
